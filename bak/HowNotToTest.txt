I've been teaching developers how to debug and test for at least ten years. They have something in common with the population at large: if they've never had to conduct tests as part of their work, they just don't know how to. 

To share how to test a system, I need an example, and it can't be technical. In the spirit of sharing, I'm going to point out some errors and biases in this article from vice.com called "Your Phone Is Listening and it's Not Paranoia". 

Around 2017, people started mentioning that they suspected Facebook's smartphone app of listening to their conversations to serve targeted ads. I'm sure you've heard it by now - maybe even experienced the phenomenon yourself. Specifics vary, but they all go something like this: two people are talking near a phone, and then later one of them sees an ad related to the same subject.

Forbes, Wired, Fast Company and more wrote articles judging phones listening impossible to rule out, but unlikely. But inevitably this article comes up as proof that someone did an "experiment" and the matter is closed. 

Author Sam Nichols makes several critical mistakes, all confined to one paragraph:

"With this in mind, I decided to try an experiment."

This is not an experiment, this would be incomplete, because there is no stated possibility of failure. A test that does not fail is called "evergreen", and it's worse than no test at all, because it gives a false sense of security. 

When I teach, I tell students to prove it. The procedures Sam describes here don't prove anything, because there is no possibility of failure.

"Twice a day for five days, I tried saying a bunch of phrases that could theoretically be used as triggers."

If you look at the picture in the article, Sam is standing in the bathtub as if it's a vocal isolation booth, holding his phone to his mouth. But in the article, he specifically mentions "sitting at a bar, iPhones in pockets." Repeating the intervals on a schedule appears to give some intention of statistical sampling, as if maybe ads would be different in the morning and at night? But its actual purpose is to pad the numbers.

Contriving conditions in order to get favorable results does not bode well for long term correctness. Both the hypothesis and its failure condition should get equal attention. 

"Phrases like Iâ€™m thinking about going back to uni and I need some cheap shirts for work. Then I carefully monitored the sponsored posts on Facebook for any changes."

The error here is insufficient data transparency. You need a baseline prior to the experiment, or else you won't know what normal is. And you can not rely on yourself to "carefully monitor" anything. You need a record of the traffic, because the data is the proof. 

"The changes came literally overnight."

In addition to the full results data, the other data we need and are not being given is the full list of search phrases. The method outlined in the last sentence indicates "a bunch" of phrases were repeated, but how many makes all the difference. Making up numbers to illustrate: 7 hits out of 10 attempts sounds promising until you find out that's 7 ads served out of 1,250 during the same period. If you don't state failure conditions, you don't know what to do when you get partial results. 

23 out of 30 and 7 out of 30 can look very similar when you're at #9. Define a threshold for acceptable results and then... prove it.

The fatal error is one of double ommision: failing to mention the part intentionally being left out. See, if you don't leave some test cases uninvestigated, then you have no comparison to prove that it wasn't all complete coincidence. 

Sam has no control group. 

It's possible those ads would have been served whether Sam whispered into his phone or not. Without a control group, that isolates the variable you're testing for, you can't exclude other theories that fit the same data. Or as my buddy posted on Facebook recently:

"Dreamed about Jimi Hendrix last night, now I'm getting ads in my feed. Is FB in my head?!?!?!"

In order for me to consider these results meaningful, you'd have to take half of those trigger phrases, *not* say them into your phone, and then go look for results. Results in your control group are called false positives, and they indicate your test is not specific enough. 

And finally, I mentioned a bias. All of these actions stem from a desire to influence the outcome of the test. But that's a guaranteed road map to failure. The point of testing a system is to learn about it. 

In order to learn anything, you must admit you do not know it all. And that's the bias developers overcome when they realize how little they know for sure about their systems, and how much they need to prove. 
